{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanikamal/deep-learning-atoz/blob/master/pytorch_example/Optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBehvqZ8znsy",
        "colab_type": "text"
      },
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCdIqY0tKbvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting seeds to try and ensure we have the same results - this is not guaranteed across PyTorch releases.\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PCJzXv0OK1Bs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "307aa085-37db-4e95-b695-5c21820d5c74"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "# Create a transform and normalise data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                              ])\n",
        "\n",
        "# Download FMNIST training dataset and load training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download FMNIST test dataset and load test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 98304/26421880 [00:00<00:27, 943938.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:00, 84222659.69it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 569331.72it/s]\n",
            " 10%|â–‰         | 425984/4422102 [00:00<00:00, 4236381.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:00, 31021326.60it/s]                           \n",
            "8192it [00:00, 199940.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZpZ12MrEDZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqMqFbIVrbFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FMNIST(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    \n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    \n",
        "    return x\n",
        "    \n",
        "#model = FMNIST()   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m68OeMRdEF0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c0QgxCF3fD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjBut_7lhAc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2ZAGFzFEQA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iPQek2nz2yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnVwV-CERd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roihp-kN0Jw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvbHIyPSEUPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtP3nCEQEUMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwcPkxQwEfYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nf2WdmP5Gst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "a7022567-11dc-4da6-da54-b203f21fd337"
      },
      "source": [
        "output = model(images)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-0.0004, -0.0004, -0.0004,  ..., -0.0007, -0.0006, -0.0004],\n",
            "        [ 0.0069,  0.0069,  0.0069,  ...,  0.0072,  0.0070,  0.0069],\n",
            "        [-0.0015, -0.0015, -0.0015,  ..., -0.0016, -0.0015, -0.0015],\n",
            "        ...,\n",
            "        [ 0.0018,  0.0018,  0.0018,  ...,  0.0017,  0.0017,  0.0018],\n",
            "        [ 0.0019,  0.0019,  0.0019,  ...,  0.0019,  0.0019,  0.0019],\n",
            "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0016,  0.0017,  0.0017]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arwzAK-1EkEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD-u49yzEj6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuGKi_nq6P0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "02a60e91-616a-44c3-b803-d8d28c47438a"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0151, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0285,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-0.0004, -0.0004, -0.0004,  ..., -0.0007, -0.0006, -0.0004],\n",
            "        [ 0.0069,  0.0069,  0.0069,  ...,  0.0072,  0.0070,  0.0069],\n",
            "        [-0.0015, -0.0015, -0.0015,  ..., -0.0016, -0.0015, -0.0015],\n",
            "        ...,\n",
            "        [ 0.0018,  0.0018,  0.0018,  ...,  0.0017,  0.0017,  0.0018],\n",
            "        [ 0.0019,  0.0019,  0.0019,  ...,  0.0019,  0.0019,  0.0019],\n",
            "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0016,  0.0017,  0.0017]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8oIy5SkEpDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnfpzGigEpAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EniqxHDwDa8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "490eb18e-abac-4212-e14a-a79729497a0d"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0151, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0285,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DViAViGEwyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGZhQE3tDcqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e66053de-bfc2-4c2c-dd4c-a065be3ec50c"
      },
      "source": [
        "model = FMNIST()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    cum_loss = 0\n",
        "    batch_num=0\n",
        "\n",
        "    for batch_num,(images, labels) in enumerate(trainloader,1):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cum_loss += loss.item()\n",
        "        print(f'Batch : {batch_num}, Loss : {loss.item()}')\n",
        "     \n",
        "    print(f\"Training loss: {cum_loss/len(trainloader)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch : 1, Loss : 2.3072664737701416\n",
            "Batch : 2, Loss : 2.288365125656128\n",
            "Batch : 3, Loss : 2.3049063682556152\n",
            "Batch : 4, Loss : 2.3015007972717285\n",
            "Batch : 5, Loss : 2.278282642364502\n",
            "Batch : 6, Loss : 2.277097702026367\n",
            "Batch : 7, Loss : 2.2820229530334473\n",
            "Batch : 8, Loss : 2.302107810974121\n",
            "Batch : 9, Loss : 2.2785263061523438\n",
            "Batch : 10, Loss : 2.27801513671875\n",
            "Batch : 11, Loss : 2.2610418796539307\n",
            "Batch : 12, Loss : 2.2764644622802734\n",
            "Batch : 13, Loss : 2.2731387615203857\n",
            "Batch : 14, Loss : 2.2578248977661133\n",
            "Batch : 15, Loss : 2.2600483894348145\n",
            "Batch : 16, Loss : 2.261315107345581\n",
            "Batch : 17, Loss : 2.2721023559570312\n",
            "Batch : 18, Loss : 2.24147629737854\n",
            "Batch : 19, Loss : 2.2405261993408203\n",
            "Batch : 20, Loss : 2.235086441040039\n",
            "Batch : 21, Loss : 2.2544260025024414\n",
            "Batch : 22, Loss : 2.2362046241760254\n",
            "Batch : 23, Loss : 2.231415271759033\n",
            "Batch : 24, Loss : 2.2389469146728516\n",
            "Batch : 25, Loss : 2.2141687870025635\n",
            "Batch : 26, Loss : 2.233987331390381\n",
            "Batch : 27, Loss : 2.2178635597229004\n",
            "Batch : 28, Loss : 2.230189323425293\n",
            "Batch : 29, Loss : 2.2189369201660156\n",
            "Batch : 30, Loss : 2.2120749950408936\n",
            "Batch : 31, Loss : 2.212344169616699\n",
            "Batch : 32, Loss : 2.208530902862549\n",
            "Batch : 33, Loss : 2.18330717086792\n",
            "Batch : 34, Loss : 2.1965603828430176\n",
            "Batch : 35, Loss : 2.181426763534546\n",
            "Batch : 36, Loss : 2.2011210918426514\n",
            "Batch : 37, Loss : 2.196009874343872\n",
            "Batch : 38, Loss : 2.159069061279297\n",
            "Batch : 39, Loss : 2.1497068405151367\n",
            "Batch : 40, Loss : 2.1899259090423584\n",
            "Batch : 41, Loss : 2.164796829223633\n",
            "Batch : 42, Loss : 2.17661452293396\n",
            "Batch : 43, Loss : 2.15094256401062\n",
            "Batch : 44, Loss : 2.1727468967437744\n",
            "Batch : 45, Loss : 2.149967908859253\n",
            "Batch : 46, Loss : 2.171332359313965\n",
            "Batch : 47, Loss : 2.1695690155029297\n",
            "Batch : 48, Loss : 2.1276767253875732\n",
            "Batch : 49, Loss : 2.138857126235962\n",
            "Batch : 50, Loss : 2.119607925415039\n",
            "Batch : 51, Loss : 2.155275583267212\n",
            "Batch : 52, Loss : 2.106316328048706\n",
            "Batch : 53, Loss : 2.1626839637756348\n",
            "Batch : 54, Loss : 2.143470287322998\n",
            "Batch : 55, Loss : 2.122385025024414\n",
            "Batch : 56, Loss : 2.1170125007629395\n",
            "Batch : 57, Loss : 2.114706039428711\n",
            "Batch : 58, Loss : 2.1105237007141113\n",
            "Batch : 59, Loss : 2.1023786067962646\n",
            "Batch : 60, Loss : 2.1150588989257812\n",
            "Batch : 61, Loss : 2.1015803813934326\n",
            "Batch : 62, Loss : 2.099888324737549\n",
            "Batch : 63, Loss : 2.090914726257324\n",
            "Batch : 64, Loss : 2.1206960678100586\n",
            "Batch : 65, Loss : 2.1207191944122314\n",
            "Batch : 66, Loss : 2.0598037242889404\n",
            "Batch : 67, Loss : 2.088256359100342\n",
            "Batch : 68, Loss : 2.083505868911743\n",
            "Batch : 69, Loss : 2.0621414184570312\n",
            "Batch : 70, Loss : 2.07832407951355\n",
            "Batch : 71, Loss : 2.049839735031128\n",
            "Batch : 72, Loss : 2.101393699645996\n",
            "Batch : 73, Loss : 2.0119125843048096\n",
            "Batch : 74, Loss : 2.0472755432128906\n",
            "Batch : 75, Loss : 2.0591607093811035\n",
            "Batch : 76, Loss : 2.0145463943481445\n",
            "Batch : 77, Loss : 2.0132009983062744\n",
            "Batch : 78, Loss : 2.0692832469940186\n",
            "Batch : 79, Loss : 1.9742687940597534\n",
            "Batch : 80, Loss : 2.001490592956543\n",
            "Batch : 81, Loss : 2.0196759700775146\n",
            "Batch : 82, Loss : 2.000135660171509\n",
            "Batch : 83, Loss : 1.9519866704940796\n",
            "Batch : 84, Loss : 1.9761892557144165\n",
            "Batch : 85, Loss : 1.985669732093811\n",
            "Batch : 86, Loss : 1.9579249620437622\n",
            "Batch : 87, Loss : 1.9509353637695312\n",
            "Batch : 88, Loss : 1.9740544557571411\n",
            "Batch : 89, Loss : 1.997693657875061\n",
            "Batch : 90, Loss : 1.9830445051193237\n",
            "Batch : 91, Loss : 1.9519984722137451\n",
            "Batch : 92, Loss : 1.995814323425293\n",
            "Batch : 93, Loss : 1.882996916770935\n",
            "Batch : 94, Loss : 1.9355740547180176\n",
            "Batch : 95, Loss : 1.9342941045761108\n",
            "Batch : 96, Loss : 1.950761318206787\n",
            "Batch : 97, Loss : 1.9132921695709229\n",
            "Batch : 98, Loss : 1.8462460041046143\n",
            "Batch : 99, Loss : 1.9494209289550781\n",
            "Batch : 100, Loss : 1.9408873319625854\n",
            "Batch : 101, Loss : 1.8907982110977173\n",
            "Batch : 102, Loss : 1.8606395721435547\n",
            "Batch : 103, Loss : 1.9013186693191528\n",
            "Batch : 104, Loss : 1.888655424118042\n",
            "Batch : 105, Loss : 1.8175097703933716\n",
            "Batch : 106, Loss : 1.8422013521194458\n",
            "Batch : 107, Loss : 1.8924274444580078\n",
            "Batch : 108, Loss : 1.8633681535720825\n",
            "Batch : 109, Loss : 1.8070482015609741\n",
            "Batch : 110, Loss : 1.8510230779647827\n",
            "Batch : 111, Loss : 1.864001989364624\n",
            "Batch : 112, Loss : 1.8724989891052246\n",
            "Batch : 113, Loss : 1.774314284324646\n",
            "Batch : 114, Loss : 1.7906323671340942\n",
            "Batch : 115, Loss : 1.7556573152542114\n",
            "Batch : 116, Loss : 1.8000102043151855\n",
            "Batch : 117, Loss : 1.8129258155822754\n",
            "Batch : 118, Loss : 1.7571617364883423\n",
            "Batch : 119, Loss : 1.7813451290130615\n",
            "Batch : 120, Loss : 1.7195900678634644\n",
            "Batch : 121, Loss : 1.7963535785675049\n",
            "Batch : 122, Loss : 1.8260608911514282\n",
            "Batch : 123, Loss : 1.7012099027633667\n",
            "Batch : 124, Loss : 1.8089656829833984\n",
            "Batch : 125, Loss : 1.7685575485229492\n",
            "Batch : 126, Loss : 1.6843540668487549\n",
            "Batch : 127, Loss : 1.7526158094406128\n",
            "Batch : 128, Loss : 1.681960105895996\n",
            "Batch : 129, Loss : 1.6977235078811646\n",
            "Batch : 130, Loss : 1.676612377166748\n",
            "Batch : 131, Loss : 1.7065377235412598\n",
            "Batch : 132, Loss : 1.6822445392608643\n",
            "Batch : 133, Loss : 1.6404788494110107\n",
            "Batch : 134, Loss : 1.6830875873565674\n",
            "Batch : 135, Loss : 1.7521443367004395\n",
            "Batch : 136, Loss : 1.7205088138580322\n",
            "Batch : 137, Loss : 1.615026831626892\n",
            "Batch : 138, Loss : 1.6726666688919067\n",
            "Batch : 139, Loss : 1.6757068634033203\n",
            "Batch : 140, Loss : 1.6011085510253906\n",
            "Batch : 141, Loss : 1.6484204530715942\n",
            "Batch : 142, Loss : 1.570003628730774\n",
            "Batch : 143, Loss : 1.651314377784729\n",
            "Batch : 144, Loss : 1.571362853050232\n",
            "Batch : 145, Loss : 1.5436737537384033\n",
            "Batch : 146, Loss : 1.5438709259033203\n",
            "Batch : 147, Loss : 1.557283878326416\n",
            "Batch : 148, Loss : 1.5364141464233398\n",
            "Batch : 149, Loss : 1.5794423818588257\n",
            "Batch : 150, Loss : 1.569944977760315\n",
            "Batch : 151, Loss : 1.508849859237671\n",
            "Batch : 152, Loss : 1.6017690896987915\n",
            "Batch : 153, Loss : 1.5836806297302246\n",
            "Batch : 154, Loss : 1.5391583442687988\n",
            "Batch : 155, Loss : 1.5011050701141357\n",
            "Batch : 156, Loss : 1.6004637479782104\n",
            "Batch : 157, Loss : 1.514003872871399\n",
            "Batch : 158, Loss : 1.6279891729354858\n",
            "Batch : 159, Loss : 1.5362401008605957\n",
            "Batch : 160, Loss : 1.421647548675537\n",
            "Batch : 161, Loss : 1.4774826765060425\n",
            "Batch : 162, Loss : 1.4398096799850464\n",
            "Batch : 163, Loss : 1.3730345964431763\n",
            "Batch : 164, Loss : 1.49296236038208\n",
            "Batch : 165, Loss : 1.5012770891189575\n",
            "Batch : 166, Loss : 1.5578879117965698\n",
            "Batch : 167, Loss : 1.4496424198150635\n",
            "Batch : 168, Loss : 1.4279388189315796\n",
            "Batch : 169, Loss : 1.4146183729171753\n",
            "Batch : 170, Loss : 1.509751558303833\n",
            "Batch : 171, Loss : 1.453037142753601\n",
            "Batch : 172, Loss : 1.5306410789489746\n",
            "Batch : 173, Loss : 1.4319969415664673\n",
            "Batch : 174, Loss : 1.4024416208267212\n",
            "Batch : 175, Loss : 1.356746792793274\n",
            "Batch : 176, Loss : 1.4077779054641724\n",
            "Batch : 177, Loss : 1.518044352531433\n",
            "Batch : 178, Loss : 1.42928147315979\n",
            "Batch : 179, Loss : 1.360162377357483\n",
            "Batch : 180, Loss : 1.34250009059906\n",
            "Batch : 181, Loss : 1.3392082452774048\n",
            "Batch : 182, Loss : 1.364702820777893\n",
            "Batch : 183, Loss : 1.418899655342102\n",
            "Batch : 184, Loss : 1.394244909286499\n",
            "Batch : 185, Loss : 1.530698299407959\n",
            "Batch : 186, Loss : 1.3058137893676758\n",
            "Batch : 187, Loss : 1.2785077095031738\n",
            "Batch : 188, Loss : 1.3217580318450928\n",
            "Batch : 189, Loss : 1.3140453100204468\n",
            "Batch : 190, Loss : 1.451682686805725\n",
            "Batch : 191, Loss : 1.3721764087677002\n",
            "Batch : 192, Loss : 1.3867686986923218\n",
            "Batch : 193, Loss : 1.3851194381713867\n",
            "Batch : 194, Loss : 1.3192718029022217\n",
            "Batch : 195, Loss : 1.1872117519378662\n",
            "Batch : 196, Loss : 1.1685653924942017\n",
            "Batch : 197, Loss : 1.3811845779418945\n",
            "Batch : 198, Loss : 1.2313885688781738\n",
            "Batch : 199, Loss : 1.2900452613830566\n",
            "Batch : 200, Loss : 1.232414722442627\n",
            "Batch : 201, Loss : 1.2762333154678345\n",
            "Batch : 202, Loss : 1.3471421003341675\n",
            "Batch : 203, Loss : 1.2543368339538574\n",
            "Batch : 204, Loss : 1.2662122249603271\n",
            "Batch : 205, Loss : 1.2558459043502808\n",
            "Batch : 206, Loss : 1.301846981048584\n",
            "Batch : 207, Loss : 1.2549864053726196\n",
            "Batch : 208, Loss : 1.214761734008789\n",
            "Batch : 209, Loss : 1.1561081409454346\n",
            "Batch : 210, Loss : 1.1935949325561523\n",
            "Batch : 211, Loss : 1.3333455324172974\n",
            "Batch : 212, Loss : 1.1499519348144531\n",
            "Batch : 213, Loss : 1.226910948753357\n",
            "Batch : 214, Loss : 1.2055678367614746\n",
            "Batch : 215, Loss : 1.1905691623687744\n",
            "Batch : 216, Loss : 1.2567009925842285\n",
            "Batch : 217, Loss : 1.293336033821106\n",
            "Batch : 218, Loss : 1.2729895114898682\n",
            "Batch : 219, Loss : 1.1889132261276245\n",
            "Batch : 220, Loss : 1.2217457294464111\n",
            "Batch : 221, Loss : 1.1029071807861328\n",
            "Batch : 222, Loss : 1.1430230140686035\n",
            "Batch : 223, Loss : 1.1383599042892456\n",
            "Batch : 224, Loss : 1.1079496145248413\n",
            "Batch : 225, Loss : 1.115022897720337\n",
            "Batch : 226, Loss : 1.2556616067886353\n",
            "Batch : 227, Loss : 1.1709617376327515\n",
            "Batch : 228, Loss : 1.317028284072876\n",
            "Batch : 229, Loss : 1.2025264501571655\n",
            "Batch : 230, Loss : 1.1310454607009888\n",
            "Batch : 231, Loss : 1.1437820196151733\n",
            "Batch : 232, Loss : 1.2180910110473633\n",
            "Batch : 233, Loss : 1.109780192375183\n",
            "Batch : 234, Loss : 1.178292155265808\n",
            "Batch : 235, Loss : 1.1602426767349243\n",
            "Batch : 236, Loss : 1.059375524520874\n",
            "Batch : 237, Loss : 1.213047742843628\n",
            "Batch : 238, Loss : 1.134635329246521\n",
            "Batch : 239, Loss : 1.0951236486434937\n",
            "Batch : 240, Loss : 1.144999623298645\n",
            "Batch : 241, Loss : 1.1075793504714966\n",
            "Batch : 242, Loss : 1.2723548412322998\n",
            "Batch : 243, Loss : 1.2597424983978271\n",
            "Batch : 244, Loss : 1.180441975593567\n",
            "Batch : 245, Loss : 1.0977139472961426\n",
            "Batch : 246, Loss : 1.089946985244751\n",
            "Batch : 247, Loss : 1.030632495880127\n",
            "Batch : 248, Loss : 1.2047557830810547\n",
            "Batch : 249, Loss : 1.0198941230773926\n",
            "Batch : 250, Loss : 1.0587043762207031\n",
            "Batch : 251, Loss : 1.0523439645767212\n",
            "Batch : 252, Loss : 1.0425925254821777\n",
            "Batch : 253, Loss : 1.0443549156188965\n",
            "Batch : 254, Loss : 0.9958981275558472\n",
            "Batch : 255, Loss : 1.1265822649002075\n",
            "Batch : 256, Loss : 1.1265718936920166\n",
            "Batch : 257, Loss : 1.0676978826522827\n",
            "Batch : 258, Loss : 0.9820785522460938\n",
            "Batch : 259, Loss : 1.0893367528915405\n",
            "Batch : 260, Loss : 1.0720276832580566\n",
            "Batch : 261, Loss : 1.0170844793319702\n",
            "Batch : 262, Loss : 1.0822778940200806\n",
            "Batch : 263, Loss : 1.126015543937683\n",
            "Batch : 264, Loss : 1.1336654424667358\n",
            "Batch : 265, Loss : 1.0612192153930664\n",
            "Batch : 266, Loss : 1.0314555168151855\n",
            "Batch : 267, Loss : 0.9730390310287476\n",
            "Batch : 268, Loss : 1.0460569858551025\n",
            "Batch : 269, Loss : 1.109437108039856\n",
            "Batch : 270, Loss : 0.942453920841217\n",
            "Batch : 271, Loss : 1.0871316194534302\n",
            "Batch : 272, Loss : 1.1134552955627441\n",
            "Batch : 273, Loss : 1.1059632301330566\n",
            "Batch : 274, Loss : 0.9519410729408264\n",
            "Batch : 275, Loss : 0.9635065197944641\n",
            "Batch : 276, Loss : 1.0294185876846313\n",
            "Batch : 277, Loss : 1.168448805809021\n",
            "Batch : 278, Loss : 1.1614445447921753\n",
            "Batch : 279, Loss : 0.9837824106216431\n",
            "Batch : 280, Loss : 0.9699403643608093\n",
            "Batch : 281, Loss : 1.1427992582321167\n",
            "Batch : 282, Loss : 0.8685054183006287\n",
            "Batch : 283, Loss : 0.8392716646194458\n",
            "Batch : 284, Loss : 1.0241713523864746\n",
            "Batch : 285, Loss : 0.9343221783638\n",
            "Batch : 286, Loss : 1.0498000383377075\n",
            "Batch : 287, Loss : 1.0773643255233765\n",
            "Batch : 288, Loss : 1.0138649940490723\n",
            "Batch : 289, Loss : 1.0758297443389893\n",
            "Batch : 290, Loss : 1.0646799802780151\n",
            "Batch : 291, Loss : 0.9058296084403992\n",
            "Batch : 292, Loss : 0.9696571826934814\n",
            "Batch : 293, Loss : 1.0727152824401855\n",
            "Batch : 294, Loss : 0.8240320682525635\n",
            "Batch : 295, Loss : 0.978036105632782\n",
            "Batch : 296, Loss : 1.0660444498062134\n",
            "Batch : 297, Loss : 0.9996482133865356\n",
            "Batch : 298, Loss : 1.051655888557434\n",
            "Batch : 299, Loss : 0.9114245176315308\n",
            "Batch : 300, Loss : 0.9215263724327087\n",
            "Batch : 301, Loss : 0.9720901250839233\n",
            "Batch : 302, Loss : 1.0234333276748657\n",
            "Batch : 303, Loss : 0.8493381142616272\n",
            "Batch : 304, Loss : 0.8582993745803833\n",
            "Batch : 305, Loss : 0.9090602397918701\n",
            "Batch : 306, Loss : 1.0527396202087402\n",
            "Batch : 307, Loss : 1.0491598844528198\n",
            "Batch : 308, Loss : 0.9200174808502197\n",
            "Batch : 309, Loss : 0.9986165165901184\n",
            "Batch : 310, Loss : 0.9688748121261597\n",
            "Batch : 311, Loss : 0.9578298926353455\n",
            "Batch : 312, Loss : 0.9689343571662903\n",
            "Batch : 313, Loss : 0.8711983561515808\n",
            "Batch : 314, Loss : 0.9280243515968323\n",
            "Batch : 315, Loss : 0.94105464220047\n",
            "Batch : 316, Loss : 0.9093226790428162\n",
            "Batch : 317, Loss : 0.9657814502716064\n",
            "Batch : 318, Loss : 0.8934503793716431\n",
            "Batch : 319, Loss : 1.0389132499694824\n",
            "Batch : 320, Loss : 0.8316051363945007\n",
            "Batch : 321, Loss : 1.1061924695968628\n",
            "Batch : 322, Loss : 1.1641438007354736\n",
            "Batch : 323, Loss : 1.017019510269165\n",
            "Batch : 324, Loss : 0.8632963299751282\n",
            "Batch : 325, Loss : 0.9171529412269592\n",
            "Batch : 326, Loss : 0.8784530162811279\n",
            "Batch : 327, Loss : 0.896760106086731\n",
            "Batch : 328, Loss : 0.8556399345397949\n",
            "Batch : 329, Loss : 0.8671011328697205\n",
            "Batch : 330, Loss : 0.8803449869155884\n",
            "Batch : 331, Loss : 0.9032301902770996\n",
            "Batch : 332, Loss : 1.0007147789001465\n",
            "Batch : 333, Loss : 0.9165624380111694\n",
            "Batch : 334, Loss : 1.0124567747116089\n",
            "Batch : 335, Loss : 0.9100622534751892\n",
            "Batch : 336, Loss : 0.9086293578147888\n",
            "Batch : 337, Loss : 0.826098620891571\n",
            "Batch : 338, Loss : 0.8282187581062317\n",
            "Batch : 339, Loss : 0.923349142074585\n",
            "Batch : 340, Loss : 0.8026499152183533\n",
            "Batch : 341, Loss : 0.8284263014793396\n",
            "Batch : 342, Loss : 0.7605389356613159\n",
            "Batch : 343, Loss : 0.8705878257751465\n",
            "Batch : 344, Loss : 0.8520339727401733\n",
            "Batch : 345, Loss : 0.7647964358329773\n",
            "Batch : 346, Loss : 0.9043696522712708\n",
            "Batch : 347, Loss : 0.8804535865783691\n",
            "Batch : 348, Loss : 0.9027996063232422\n",
            "Batch : 349, Loss : 0.8458782434463501\n",
            "Batch : 350, Loss : 0.7979927659034729\n",
            "Batch : 351, Loss : 0.878275454044342\n",
            "Batch : 352, Loss : 0.8830075263977051\n",
            "Batch : 353, Loss : 0.8684966564178467\n",
            "Batch : 354, Loss : 0.8152624368667603\n",
            "Batch : 355, Loss : 0.7873918414115906\n",
            "Batch : 356, Loss : 0.8207087516784668\n",
            "Batch : 357, Loss : 0.8063499331474304\n",
            "Batch : 358, Loss : 0.9580703973770142\n",
            "Batch : 359, Loss : 0.8938491344451904\n",
            "Batch : 360, Loss : 0.8493979573249817\n",
            "Batch : 361, Loss : 0.7761769890785217\n",
            "Batch : 362, Loss : 0.9212705492973328\n",
            "Batch : 363, Loss : 0.8977428078651428\n",
            "Batch : 364, Loss : 0.823185384273529\n",
            "Batch : 365, Loss : 0.7342696785926819\n",
            "Batch : 366, Loss : 0.9282717704772949\n",
            "Batch : 367, Loss : 0.9211367964744568\n",
            "Batch : 368, Loss : 1.0930038690567017\n",
            "Batch : 369, Loss : 0.8094150424003601\n",
            "Batch : 370, Loss : 0.8239569664001465\n",
            "Batch : 371, Loss : 0.9299612641334534\n",
            "Batch : 372, Loss : 0.9884332418441772\n",
            "Batch : 373, Loss : 0.8688575625419617\n",
            "Batch : 374, Loss : 0.9317646026611328\n",
            "Batch : 375, Loss : 1.0074976682662964\n",
            "Batch : 376, Loss : 0.7230298519134521\n",
            "Batch : 377, Loss : 0.9265598058700562\n",
            "Batch : 378, Loss : 0.7993811368942261\n",
            "Batch : 379, Loss : 0.8453940749168396\n",
            "Batch : 380, Loss : 0.8845803737640381\n",
            "Batch : 381, Loss : 0.7797443270683289\n",
            "Batch : 382, Loss : 0.843971312046051\n",
            "Batch : 383, Loss : 0.9713019132614136\n",
            "Batch : 384, Loss : 0.8521347045898438\n",
            "Batch : 385, Loss : 0.887802004814148\n",
            "Batch : 386, Loss : 0.8182075619697571\n",
            "Batch : 387, Loss : 0.8273245096206665\n",
            "Batch : 388, Loss : 0.7920876145362854\n",
            "Batch : 389, Loss : 1.0852773189544678\n",
            "Batch : 390, Loss : 0.9102338552474976\n",
            "Batch : 391, Loss : 0.870733380317688\n",
            "Batch : 392, Loss : 0.8671579957008362\n",
            "Batch : 393, Loss : 0.9126368165016174\n",
            "Batch : 394, Loss : 0.7354848384857178\n",
            "Batch : 395, Loss : 0.8450465798377991\n",
            "Batch : 396, Loss : 0.8794946670532227\n",
            "Batch : 397, Loss : 0.8245576620101929\n",
            "Batch : 398, Loss : 0.7831891775131226\n",
            "Batch : 399, Loss : 0.8343012928962708\n",
            "Batch : 400, Loss : 0.8914602398872375\n",
            "Batch : 401, Loss : 0.8135738372802734\n",
            "Batch : 402, Loss : 0.8037071824073792\n",
            "Batch : 403, Loss : 0.8829105496406555\n",
            "Batch : 404, Loss : 0.7963433265686035\n",
            "Batch : 405, Loss : 0.9002366065979004\n",
            "Batch : 406, Loss : 0.7158210277557373\n",
            "Batch : 407, Loss : 0.9091520309448242\n",
            "Batch : 408, Loss : 0.7662600874900818\n",
            "Batch : 409, Loss : 0.7814144492149353\n",
            "Batch : 410, Loss : 0.842713475227356\n",
            "Batch : 411, Loss : 0.7641225457191467\n",
            "Batch : 412, Loss : 0.7864933013916016\n",
            "Batch : 413, Loss : 0.7474683523178101\n",
            "Batch : 414, Loss : 0.6577435731887817\n",
            "Batch : 415, Loss : 0.8480470180511475\n",
            "Batch : 416, Loss : 0.7902114987373352\n",
            "Batch : 417, Loss : 0.8062775731086731\n",
            "Batch : 418, Loss : 0.8339020609855652\n",
            "Batch : 419, Loss : 0.7854911088943481\n",
            "Batch : 420, Loss : 0.8026905059814453\n",
            "Batch : 421, Loss : 0.9773293733596802\n",
            "Batch : 422, Loss : 0.7197680473327637\n",
            "Batch : 423, Loss : 0.852682888507843\n",
            "Batch : 424, Loss : 0.6799079179763794\n",
            "Batch : 425, Loss : 0.8209723830223083\n",
            "Batch : 426, Loss : 0.7839114665985107\n",
            "Batch : 427, Loss : 0.7291870713233948\n",
            "Batch : 428, Loss : 0.8702354431152344\n",
            "Batch : 429, Loss : 0.7280413508415222\n",
            "Batch : 430, Loss : 0.7339928150177002\n",
            "Batch : 431, Loss : 0.8574195504188538\n",
            "Batch : 432, Loss : 0.8005540370941162\n",
            "Batch : 433, Loss : 0.7386952638626099\n",
            "Batch : 434, Loss : 0.8094178438186646\n",
            "Batch : 435, Loss : 0.7776733636856079\n",
            "Batch : 436, Loss : 0.8416812419891357\n",
            "Batch : 437, Loss : 0.6802396774291992\n",
            "Batch : 438, Loss : 0.8614040613174438\n",
            "Batch : 439, Loss : 0.8386146426200867\n",
            "Batch : 440, Loss : 0.7010174989700317\n",
            "Batch : 441, Loss : 0.9777505397796631\n",
            "Batch : 442, Loss : 0.8315463662147522\n",
            "Batch : 443, Loss : 0.5960734486579895\n",
            "Batch : 444, Loss : 0.8287420868873596\n",
            "Batch : 445, Loss : 0.6986141800880432\n",
            "Batch : 446, Loss : 0.6867812275886536\n",
            "Batch : 447, Loss : 0.8329375982284546\n",
            "Batch : 448, Loss : 0.951930582523346\n",
            "Batch : 449, Loss : 1.013325810432434\n",
            "Batch : 450, Loss : 0.8764013648033142\n",
            "Batch : 451, Loss : 0.6130234003067017\n",
            "Batch : 452, Loss : 0.8226187825202942\n",
            "Batch : 453, Loss : 0.8953377604484558\n",
            "Batch : 454, Loss : 0.9911094903945923\n",
            "Batch : 455, Loss : 0.6610313057899475\n",
            "Batch : 456, Loss : 0.831813395023346\n",
            "Batch : 457, Loss : 0.7341846823692322\n",
            "Batch : 458, Loss : 0.8478311896324158\n",
            "Batch : 459, Loss : 0.7626640200614929\n",
            "Batch : 460, Loss : 0.7000948190689087\n",
            "Batch : 461, Loss : 0.901291012763977\n",
            "Batch : 462, Loss : 0.7903916239738464\n",
            "Batch : 463, Loss : 0.7858090996742249\n",
            "Batch : 464, Loss : 0.725614070892334\n",
            "Batch : 465, Loss : 0.6941235065460205\n",
            "Batch : 466, Loss : 0.6183124780654907\n",
            "Batch : 467, Loss : 0.5954136848449707\n",
            "Batch : 468, Loss : 0.7560441493988037\n",
            "Batch : 469, Loss : 0.8390429615974426\n",
            "Batch : 470, Loss : 0.8511555790901184\n",
            "Batch : 471, Loss : 0.7293270826339722\n",
            "Batch : 472, Loss : 0.7080274224281311\n",
            "Batch : 473, Loss : 0.7914901375770569\n",
            "Batch : 474, Loss : 0.7658334970474243\n",
            "Batch : 475, Loss : 0.7631340026855469\n",
            "Batch : 476, Loss : 0.9079618453979492\n",
            "Batch : 477, Loss : 0.7090211510658264\n",
            "Batch : 478, Loss : 0.8651057481765747\n",
            "Batch : 479, Loss : 1.0298246145248413\n",
            "Batch : 480, Loss : 0.7285561561584473\n",
            "Batch : 481, Loss : 0.7956753969192505\n",
            "Batch : 482, Loss : 0.6916142106056213\n",
            "Batch : 483, Loss : 0.781952440738678\n",
            "Batch : 484, Loss : 0.795340359210968\n",
            "Batch : 485, Loss : 0.848452091217041\n",
            "Batch : 486, Loss : 0.645694375038147\n",
            "Batch : 487, Loss : 0.7878632545471191\n",
            "Batch : 488, Loss : 0.5788811445236206\n",
            "Batch : 489, Loss : 0.9018925428390503\n",
            "Batch : 490, Loss : 1.0051320791244507\n",
            "Batch : 491, Loss : 0.8162555694580078\n",
            "Batch : 492, Loss : 0.7170444130897522\n",
            "Batch : 493, Loss : 0.9623134732246399\n",
            "Batch : 494, Loss : 0.7212498188018799\n",
            "Batch : 495, Loss : 0.8200949430465698\n",
            "Batch : 496, Loss : 0.8428405523300171\n",
            "Batch : 497, Loss : 0.8266487121582031\n",
            "Batch : 498, Loss : 0.728888988494873\n",
            "Batch : 499, Loss : 0.6201905012130737\n",
            "Batch : 500, Loss : 0.6668668389320374\n",
            "Batch : 501, Loss : 0.7001544237136841\n",
            "Batch : 502, Loss : 0.7390238642692566\n",
            "Batch : 503, Loss : 0.7694615125656128\n",
            "Batch : 504, Loss : 0.6881656050682068\n",
            "Batch : 505, Loss : 0.7486900687217712\n",
            "Batch : 506, Loss : 0.7178452014923096\n",
            "Batch : 507, Loss : 0.7645041346549988\n",
            "Batch : 508, Loss : 0.8180655837059021\n",
            "Batch : 509, Loss : 0.626116931438446\n",
            "Batch : 510, Loss : 0.5757689476013184\n",
            "Batch : 511, Loss : 0.6368713974952698\n",
            "Batch : 512, Loss : 0.8209700584411621\n",
            "Batch : 513, Loss : 0.6571944355964661\n",
            "Batch : 514, Loss : 0.8181227445602417\n",
            "Batch : 515, Loss : 0.8991817831993103\n",
            "Batch : 516, Loss : 0.5987187027931213\n",
            "Batch : 517, Loss : 0.710439145565033\n",
            "Batch : 518, Loss : 0.695527195930481\n",
            "Batch : 519, Loss : 0.7668682932853699\n",
            "Batch : 520, Loss : 0.6248462796211243\n",
            "Batch : 521, Loss : 0.7633218169212341\n",
            "Batch : 522, Loss : 0.7639679908752441\n",
            "Batch : 523, Loss : 0.6813908815383911\n",
            "Batch : 524, Loss : 0.7032680511474609\n",
            "Batch : 525, Loss : 0.815836489200592\n",
            "Batch : 526, Loss : 0.7008259296417236\n",
            "Batch : 527, Loss : 0.7318704724311829\n",
            "Batch : 528, Loss : 0.823745846748352\n",
            "Batch : 529, Loss : 0.7302277088165283\n",
            "Batch : 530, Loss : 0.8000780344009399\n",
            "Batch : 531, Loss : 0.7459021806716919\n",
            "Batch : 532, Loss : 0.6744062304496765\n",
            "Batch : 533, Loss : 0.6209844946861267\n",
            "Batch : 534, Loss : 0.6043549180030823\n",
            "Batch : 535, Loss : 0.715363621711731\n",
            "Batch : 536, Loss : 0.767081081867218\n",
            "Batch : 537, Loss : 0.715227484703064\n",
            "Batch : 538, Loss : 0.7836174368858337\n",
            "Batch : 539, Loss : 0.885968804359436\n",
            "Batch : 540, Loss : 0.5995661616325378\n",
            "Batch : 541, Loss : 0.7726279497146606\n",
            "Batch : 542, Loss : 0.8072928190231323\n",
            "Batch : 543, Loss : 0.6856866478919983\n",
            "Batch : 544, Loss : 0.8178539872169495\n",
            "Batch : 545, Loss : 0.7820209860801697\n",
            "Batch : 546, Loss : 0.6277300119400024\n",
            "Batch : 547, Loss : 0.6063002347946167\n",
            "Batch : 548, Loss : 0.5771195888519287\n",
            "Batch : 549, Loss : 0.7162109613418579\n",
            "Batch : 550, Loss : 0.7243058681488037\n",
            "Batch : 551, Loss : 0.6795232892036438\n",
            "Batch : 552, Loss : 0.6875499486923218\n",
            "Batch : 553, Loss : 0.633734405040741\n",
            "Batch : 554, Loss : 0.7657601237297058\n",
            "Batch : 555, Loss : 0.5945144891738892\n",
            "Batch : 556, Loss : 0.7037349343299866\n",
            "Batch : 557, Loss : 0.5918630361557007\n",
            "Batch : 558, Loss : 0.7345333099365234\n",
            "Batch : 559, Loss : 0.7458140254020691\n",
            "Batch : 560, Loss : 0.6451553106307983\n",
            "Batch : 561, Loss : 0.7310765981674194\n",
            "Batch : 562, Loss : 0.715886652469635\n",
            "Batch : 563, Loss : 0.5862026810646057\n",
            "Batch : 564, Loss : 0.6607756614685059\n",
            "Batch : 565, Loss : 0.7020209431648254\n",
            "Batch : 566, Loss : 0.676846981048584\n",
            "Batch : 567, Loss : 0.5683589577674866\n",
            "Batch : 568, Loss : 0.8122460246086121\n",
            "Batch : 569, Loss : 0.8262040019035339\n",
            "Batch : 570, Loss : 0.5531499981880188\n",
            "Batch : 571, Loss : 0.8177269101142883\n",
            "Batch : 572, Loss : 0.6182724833488464\n",
            "Batch : 573, Loss : 0.8577133417129517\n",
            "Batch : 574, Loss : 0.6879887580871582\n",
            "Batch : 575, Loss : 0.8024770617485046\n",
            "Batch : 576, Loss : 0.6449849009513855\n",
            "Batch : 577, Loss : 0.8956261277198792\n",
            "Batch : 578, Loss : 0.8353312015533447\n",
            "Batch : 579, Loss : 0.7226057052612305\n",
            "Batch : 580, Loss : 0.642701268196106\n",
            "Batch : 581, Loss : 0.7412430644035339\n",
            "Batch : 582, Loss : 0.8084628582000732\n",
            "Batch : 583, Loss : 0.7360824942588806\n",
            "Batch : 584, Loss : 0.7131630778312683\n",
            "Batch : 585, Loss : 0.6824695467948914\n",
            "Batch : 586, Loss : 0.8416376113891602\n",
            "Batch : 587, Loss : 0.7754369974136353\n",
            "Batch : 588, Loss : 0.6350184679031372\n",
            "Batch : 589, Loss : 0.7927233576774597\n",
            "Batch : 590, Loss : 0.61055988073349\n",
            "Batch : 591, Loss : 0.5839828848838806\n",
            "Batch : 592, Loss : 0.6542516350746155\n",
            "Batch : 593, Loss : 0.6945109963417053\n",
            "Batch : 594, Loss : 0.7460648417472839\n",
            "Batch : 595, Loss : 0.7481682896614075\n",
            "Batch : 596, Loss : 0.6829922199249268\n",
            "Batch : 597, Loss : 0.9021749496459961\n",
            "Batch : 598, Loss : 0.6492428183555603\n",
            "Batch : 599, Loss : 0.5841836333274841\n",
            "Batch : 600, Loss : 0.6647478938102722\n",
            "Batch : 601, Loss : 0.8173638582229614\n",
            "Batch : 602, Loss : 0.5498622059822083\n",
            "Batch : 603, Loss : 0.6409100890159607\n",
            "Batch : 604, Loss : 0.8024593591690063\n",
            "Batch : 605, Loss : 0.7065194845199585\n",
            "Batch : 606, Loss : 0.7493241429328918\n",
            "Batch : 607, Loss : 0.5115583539009094\n",
            "Batch : 608, Loss : 0.7923207879066467\n",
            "Batch : 609, Loss : 0.7311131954193115\n",
            "Batch : 610, Loss : 0.7547754049301147\n",
            "Batch : 611, Loss : 0.7429410219192505\n",
            "Batch : 612, Loss : 0.7045830488204956\n",
            "Batch : 613, Loss : 0.7515861988067627\n",
            "Batch : 614, Loss : 0.7577175498008728\n",
            "Batch : 615, Loss : 0.8686425685882568\n",
            "Batch : 616, Loss : 0.8833823204040527\n",
            "Batch : 617, Loss : 0.5212949514389038\n",
            "Batch : 618, Loss : 0.801815927028656\n",
            "Batch : 619, Loss : 0.6853581070899963\n",
            "Batch : 620, Loss : 0.6867835521697998\n",
            "Batch : 621, Loss : 0.838817298412323\n",
            "Batch : 622, Loss : 0.8420730829238892\n",
            "Batch : 623, Loss : 0.81810063123703\n",
            "Batch : 624, Loss : 0.8232707977294922\n",
            "Batch : 625, Loss : 0.5072008371353149\n",
            "Batch : 626, Loss : 0.7882264256477356\n",
            "Batch : 627, Loss : 0.7044596672058105\n",
            "Batch : 628, Loss : 0.8803495764732361\n",
            "Batch : 629, Loss : 0.8332690000534058\n",
            "Batch : 630, Loss : 0.6668519973754883\n",
            "Batch : 631, Loss : 0.6573176383972168\n",
            "Batch : 632, Loss : 0.7159435153007507\n",
            "Batch : 633, Loss : 0.666692316532135\n",
            "Batch : 634, Loss : 0.5520530939102173\n",
            "Batch : 635, Loss : 0.6403949856758118\n",
            "Batch : 636, Loss : 0.6086441874504089\n",
            "Batch : 637, Loss : 0.5769174098968506\n",
            "Batch : 638, Loss : 0.7067504525184631\n",
            "Batch : 639, Loss : 0.6889973878860474\n",
            "Batch : 640, Loss : 0.7404850125312805\n",
            "Batch : 641, Loss : 0.579624354839325\n",
            "Batch : 642, Loss : 0.6452949047088623\n",
            "Batch : 643, Loss : 0.660976231098175\n",
            "Batch : 644, Loss : 0.6699577569961548\n",
            "Batch : 645, Loss : 0.5862603783607483\n",
            "Batch : 646, Loss : 0.712363064289093\n",
            "Batch : 647, Loss : 0.6703590154647827\n",
            "Batch : 648, Loss : 0.5812421441078186\n",
            "Batch : 649, Loss : 0.6934175491333008\n",
            "Batch : 650, Loss : 0.7670721411705017\n",
            "Batch : 651, Loss : 0.6700173020362854\n",
            "Batch : 652, Loss : 0.5952283143997192\n",
            "Batch : 653, Loss : 0.6965495347976685\n",
            "Batch : 654, Loss : 0.674764096736908\n",
            "Batch : 655, Loss : 0.7032652497291565\n",
            "Batch : 656, Loss : 0.7302456498146057\n",
            "Batch : 657, Loss : 0.6455638408660889\n",
            "Batch : 658, Loss : 0.8688472509384155\n",
            "Batch : 659, Loss : 0.6208707094192505\n",
            "Batch : 660, Loss : 0.6904846429824829\n",
            "Batch : 661, Loss : 0.8703557848930359\n",
            "Batch : 662, Loss : 0.516952633857727\n",
            "Batch : 663, Loss : 0.7158975005149841\n",
            "Batch : 664, Loss : 0.6938869953155518\n",
            "Batch : 665, Loss : 0.7605727314949036\n",
            "Batch : 666, Loss : 0.7405463457107544\n",
            "Batch : 667, Loss : 0.5392226576805115\n",
            "Batch : 668, Loss : 0.6232938170433044\n",
            "Batch : 669, Loss : 0.7491415739059448\n",
            "Batch : 670, Loss : 0.6374930143356323\n",
            "Batch : 671, Loss : 0.6097742915153503\n",
            "Batch : 672, Loss : 0.7418579459190369\n",
            "Batch : 673, Loss : 0.6922261118888855\n",
            "Batch : 674, Loss : 0.7241102457046509\n",
            "Batch : 675, Loss : 0.733339250087738\n",
            "Batch : 676, Loss : 0.715402364730835\n",
            "Batch : 677, Loss : 0.6741828322410583\n",
            "Batch : 678, Loss : 0.7108805775642395\n",
            "Batch : 679, Loss : 0.6717433929443359\n",
            "Batch : 680, Loss : 0.5247433185577393\n",
            "Batch : 681, Loss : 0.6983130574226379\n",
            "Batch : 682, Loss : 0.582033097743988\n",
            "Batch : 683, Loss : 0.6886465549468994\n",
            "Batch : 684, Loss : 0.606579601764679\n",
            "Batch : 685, Loss : 0.7954825162887573\n",
            "Batch : 686, Loss : 0.6436346769332886\n",
            "Batch : 687, Loss : 0.5862751007080078\n",
            "Batch : 688, Loss : 0.5565547943115234\n",
            "Batch : 689, Loss : 0.5944436192512512\n",
            "Batch : 690, Loss : 0.3818419277667999\n",
            "Batch : 691, Loss : 0.6102442741394043\n",
            "Batch : 692, Loss : 0.8900747299194336\n",
            "Batch : 693, Loss : 0.7483468055725098\n",
            "Batch : 694, Loss : 0.5845595598220825\n",
            "Batch : 695, Loss : 0.7340801954269409\n",
            "Batch : 696, Loss : 0.7673280239105225\n",
            "Batch : 697, Loss : 0.7287829518318176\n",
            "Batch : 698, Loss : 0.7086018919944763\n",
            "Batch : 699, Loss : 0.5820656418800354\n",
            "Batch : 700, Loss : 0.79509437084198\n",
            "Batch : 701, Loss : 0.8860697746276855\n",
            "Batch : 702, Loss : 0.6280319690704346\n",
            "Batch : 703, Loss : 0.6109765768051147\n",
            "Batch : 704, Loss : 0.6117637157440186\n",
            "Batch : 705, Loss : 0.46953099966049194\n",
            "Batch : 706, Loss : 0.707882821559906\n",
            "Batch : 707, Loss : 0.6985654234886169\n",
            "Batch : 708, Loss : 0.6303345561027527\n",
            "Batch : 709, Loss : 0.5704299807548523\n",
            "Batch : 710, Loss : 0.8939830660820007\n",
            "Batch : 711, Loss : 0.6500504612922668\n",
            "Batch : 712, Loss : 0.7543462514877319\n",
            "Batch : 713, Loss : 0.6225394606590271\n",
            "Batch : 714, Loss : 0.5027395486831665\n",
            "Batch : 715, Loss : 0.659058153629303\n",
            "Batch : 716, Loss : 0.5006905198097229\n",
            "Batch : 717, Loss : 0.712539792060852\n",
            "Batch : 718, Loss : 0.7808569669723511\n",
            "Batch : 719, Loss : 0.6571109294891357\n",
            "Batch : 720, Loss : 0.539957582950592\n",
            "Batch : 721, Loss : 0.5487465858459473\n",
            "Batch : 722, Loss : 0.6594753861427307\n",
            "Batch : 723, Loss : 0.7550453543663025\n",
            "Batch : 724, Loss : 0.6160655617713928\n",
            "Batch : 725, Loss : 0.6684432029724121\n",
            "Batch : 726, Loss : 0.8044127225875854\n",
            "Batch : 727, Loss : 0.7366746068000793\n",
            "Batch : 728, Loss : 0.803022027015686\n",
            "Batch : 729, Loss : 0.581436038017273\n",
            "Batch : 730, Loss : 0.6982654333114624\n",
            "Batch : 731, Loss : 0.6443042159080505\n",
            "Batch : 732, Loss : 0.5783630013465881\n",
            "Batch : 733, Loss : 0.596787691116333\n",
            "Batch : 734, Loss : 0.5850093364715576\n",
            "Batch : 735, Loss : 0.5939128994941711\n",
            "Batch : 736, Loss : 0.7183687090873718\n",
            "Batch : 737, Loss : 0.5253506898880005\n",
            "Batch : 738, Loss : 0.6589367389678955\n",
            "Batch : 739, Loss : 0.801835834980011\n",
            "Batch : 740, Loss : 0.66390061378479\n",
            "Batch : 741, Loss : 0.6596943736076355\n",
            "Batch : 742, Loss : 0.7839537262916565\n",
            "Batch : 743, Loss : 0.6642364263534546\n",
            "Batch : 744, Loss : 0.7208248972892761\n",
            "Batch : 745, Loss : 0.5461099743843079\n",
            "Batch : 746, Loss : 0.6773319840431213\n",
            "Batch : 747, Loss : 0.6989395022392273\n",
            "Batch : 748, Loss : 0.5112060308456421\n",
            "Batch : 749, Loss : 0.5222391486167908\n",
            "Batch : 750, Loss : 0.5779559016227722\n",
            "Batch : 751, Loss : 0.5782549977302551\n",
            "Batch : 752, Loss : 0.7588960528373718\n",
            "Batch : 753, Loss : 0.4941640794277191\n",
            "Batch : 754, Loss : 0.5813581347465515\n",
            "Batch : 755, Loss : 0.6940640807151794\n",
            "Batch : 756, Loss : 0.6438574194908142\n",
            "Batch : 757, Loss : 0.6889375448226929\n",
            "Batch : 758, Loss : 0.7136685848236084\n",
            "Batch : 759, Loss : 0.42175447940826416\n",
            "Batch : 760, Loss : 0.48471567034721375\n",
            "Batch : 761, Loss : 0.630279004573822\n",
            "Batch : 762, Loss : 0.5917827486991882\n",
            "Batch : 763, Loss : 0.5842999815940857\n",
            "Batch : 764, Loss : 0.6085186004638672\n",
            "Batch : 765, Loss : 0.6889694333076477\n",
            "Batch : 766, Loss : 0.8334799408912659\n",
            "Batch : 767, Loss : 0.6385461688041687\n",
            "Batch : 768, Loss : 0.7174453735351562\n",
            "Batch : 769, Loss : 0.5114924907684326\n",
            "Batch : 770, Loss : 0.5146019458770752\n",
            "Batch : 771, Loss : 0.7465205788612366\n",
            "Batch : 772, Loss : 0.6496559381484985\n",
            "Batch : 773, Loss : 0.6237215399742126\n",
            "Batch : 774, Loss : 0.6969223022460938\n",
            "Batch : 775, Loss : 0.5976212620735168\n",
            "Batch : 776, Loss : 0.6702814102172852\n",
            "Batch : 777, Loss : 0.8393078446388245\n",
            "Batch : 778, Loss : 0.5727975368499756\n",
            "Batch : 779, Loss : 0.5851537585258484\n",
            "Batch : 780, Loss : 0.5914366841316223\n",
            "Batch : 781, Loss : 0.6040165424346924\n",
            "Batch : 782, Loss : 0.5973733067512512\n",
            "Batch : 783, Loss : 0.5433837175369263\n",
            "Batch : 784, Loss : 0.6109495759010315\n",
            "Batch : 785, Loss : 0.8485400080680847\n",
            "Batch : 786, Loss : 0.6460675597190857\n",
            "Batch : 787, Loss : 0.6566422581672668\n",
            "Batch : 788, Loss : 0.509881854057312\n",
            "Batch : 789, Loss : 0.491036981344223\n",
            "Batch : 790, Loss : 0.6406444311141968\n",
            "Batch : 791, Loss : 0.7735382318496704\n",
            "Batch : 792, Loss : 0.7026078104972839\n",
            "Batch : 793, Loss : 0.6989493370056152\n",
            "Batch : 794, Loss : 0.629638135433197\n",
            "Batch : 795, Loss : 0.6231385469436646\n",
            "Batch : 796, Loss : 0.5260425209999084\n",
            "Batch : 797, Loss : 0.7565866112709045\n",
            "Batch : 798, Loss : 0.7019398212432861\n",
            "Batch : 799, Loss : 0.6373399496078491\n",
            "Batch : 800, Loss : 0.5410399436950684\n",
            "Batch : 801, Loss : 0.8219149112701416\n",
            "Batch : 802, Loss : 0.5914568901062012\n",
            "Batch : 803, Loss : 0.6075119972229004\n",
            "Batch : 804, Loss : 0.6380148530006409\n",
            "Batch : 805, Loss : 0.6760457754135132\n",
            "Batch : 806, Loss : 0.5678837895393372\n",
            "Batch : 807, Loss : 0.6457022428512573\n",
            "Batch : 808, Loss : 0.6024953722953796\n",
            "Batch : 809, Loss : 0.5117679238319397\n",
            "Batch : 810, Loss : 0.6584801077842712\n",
            "Batch : 811, Loss : 0.5966051816940308\n",
            "Batch : 812, Loss : 0.7262106537818909\n",
            "Batch : 813, Loss : 0.5620126724243164\n",
            "Batch : 814, Loss : 0.910114586353302\n",
            "Batch : 815, Loss : 0.7536841034889221\n",
            "Batch : 816, Loss : 0.5922945141792297\n",
            "Batch : 817, Loss : 0.7709909081459045\n",
            "Batch : 818, Loss : 0.47825801372528076\n",
            "Batch : 819, Loss : 0.6542987823486328\n",
            "Batch : 820, Loss : 0.4775928854942322\n",
            "Batch : 821, Loss : 0.6218933463096619\n",
            "Batch : 822, Loss : 0.7646203637123108\n",
            "Batch : 823, Loss : 0.5709635019302368\n",
            "Batch : 824, Loss : 0.6634500622749329\n",
            "Batch : 825, Loss : 0.5629825592041016\n",
            "Batch : 826, Loss : 0.5487207174301147\n",
            "Batch : 827, Loss : 0.7177115082740784\n",
            "Batch : 828, Loss : 0.603378415107727\n",
            "Batch : 829, Loss : 0.545766294002533\n",
            "Batch : 830, Loss : 0.628899872303009\n",
            "Batch : 831, Loss : 0.8071840405464172\n",
            "Batch : 832, Loss : 0.5614114999771118\n",
            "Batch : 833, Loss : 0.7292526960372925\n",
            "Batch : 834, Loss : 0.6549292206764221\n",
            "Batch : 835, Loss : 0.7051754593849182\n",
            "Batch : 836, Loss : 0.5347442030906677\n",
            "Batch : 837, Loss : 0.8283076882362366\n",
            "Batch : 838, Loss : 0.5244534015655518\n",
            "Batch : 839, Loss : 0.7236166000366211\n",
            "Batch : 840, Loss : 0.5899219512939453\n",
            "Batch : 841, Loss : 0.6335191130638123\n",
            "Batch : 842, Loss : 0.8120339512825012\n",
            "Batch : 843, Loss : 0.7817937731742859\n",
            "Batch : 844, Loss : 0.5470208525657654\n",
            "Batch : 845, Loss : 0.5416996479034424\n",
            "Batch : 846, Loss : 0.4481993615627289\n",
            "Batch : 847, Loss : 0.5733529925346375\n",
            "Batch : 848, Loss : 0.5561549067497253\n",
            "Batch : 849, Loss : 0.4601844847202301\n",
            "Batch : 850, Loss : 0.45718076825141907\n",
            "Batch : 851, Loss : 0.6381653547286987\n",
            "Batch : 852, Loss : 0.5779463648796082\n",
            "Batch : 853, Loss : 0.639512300491333\n",
            "Batch : 854, Loss : 0.5783041715621948\n",
            "Batch : 855, Loss : 0.5399842858314514\n",
            "Batch : 856, Loss : 0.6445209383964539\n",
            "Batch : 857, Loss : 0.7745283842086792\n",
            "Batch : 858, Loss : 0.5757173299789429\n",
            "Batch : 859, Loss : 0.7395725250244141\n",
            "Batch : 860, Loss : 0.5012239813804626\n",
            "Batch : 861, Loss : 0.5731905698776245\n",
            "Batch : 862, Loss : 0.6356980800628662\n",
            "Batch : 863, Loss : 0.5058584809303284\n",
            "Batch : 864, Loss : 0.6225600838661194\n",
            "Batch : 865, Loss : 0.8107092380523682\n",
            "Batch : 866, Loss : 0.6923058032989502\n",
            "Batch : 867, Loss : 0.5550733208656311\n",
            "Batch : 868, Loss : 0.76503586769104\n",
            "Batch : 869, Loss : 0.6526919603347778\n",
            "Batch : 870, Loss : 0.7363432049751282\n",
            "Batch : 871, Loss : 0.5924170613288879\n",
            "Batch : 872, Loss : 0.5597836971282959\n",
            "Batch : 873, Loss : 0.7949168682098389\n",
            "Batch : 874, Loss : 0.5404435992240906\n",
            "Batch : 875, Loss : 0.6510734558105469\n",
            "Batch : 876, Loss : 0.5778486132621765\n",
            "Batch : 877, Loss : 0.5095645189285278\n",
            "Batch : 878, Loss : 0.7589080333709717\n",
            "Batch : 879, Loss : 0.7202188372612\n",
            "Batch : 880, Loss : 0.6313079595565796\n",
            "Batch : 881, Loss : 0.6881155371665955\n",
            "Batch : 882, Loss : 0.7236009836196899\n",
            "Batch : 883, Loss : 0.5517273545265198\n",
            "Batch : 884, Loss : 0.4051086902618408\n",
            "Batch : 885, Loss : 0.6411074995994568\n",
            "Batch : 886, Loss : 0.40344810485839844\n",
            "Batch : 887, Loss : 0.7129672765731812\n",
            "Batch : 888, Loss : 0.514367401599884\n",
            "Batch : 889, Loss : 0.5083722472190857\n",
            "Batch : 890, Loss : 0.4542573094367981\n",
            "Batch : 891, Loss : 0.6496537327766418\n",
            "Batch : 892, Loss : 0.7081182599067688\n",
            "Batch : 893, Loss : 0.6832848787307739\n",
            "Batch : 894, Loss : 0.5432385206222534\n",
            "Batch : 895, Loss : 0.68842613697052\n",
            "Batch : 896, Loss : 0.6121780276298523\n",
            "Batch : 897, Loss : 0.43889811635017395\n",
            "Batch : 898, Loss : 0.8522419333457947\n",
            "Batch : 899, Loss : 0.4930114448070526\n",
            "Batch : 900, Loss : 0.7273232340812683\n",
            "Batch : 901, Loss : 0.6783976554870605\n",
            "Batch : 902, Loss : 0.7158721089363098\n",
            "Batch : 903, Loss : 0.676915168762207\n",
            "Batch : 904, Loss : 0.709597110748291\n",
            "Batch : 905, Loss : 0.6534756422042847\n",
            "Batch : 906, Loss : 0.5897828340530396\n",
            "Batch : 907, Loss : 0.6723396182060242\n",
            "Batch : 908, Loss : 0.6193966269493103\n",
            "Batch : 909, Loss : 0.6186729669570923\n",
            "Batch : 910, Loss : 0.5079958438873291\n",
            "Batch : 911, Loss : 0.6109662652015686\n",
            "Batch : 912, Loss : 0.6447783708572388\n",
            "Batch : 913, Loss : 0.6352671384811401\n",
            "Batch : 914, Loss : 0.7240836024284363\n",
            "Batch : 915, Loss : 0.741016149520874\n",
            "Batch : 916, Loss : 0.5891452431678772\n",
            "Batch : 917, Loss : 0.6541052460670471\n",
            "Batch : 918, Loss : 0.45783179998397827\n",
            "Batch : 919, Loss : 0.7758436799049377\n",
            "Batch : 920, Loss : 0.5031097531318665\n",
            "Batch : 921, Loss : 0.67304927110672\n",
            "Batch : 922, Loss : 0.5100734829902649\n",
            "Batch : 923, Loss : 0.744724452495575\n",
            "Batch : 924, Loss : 0.5182812809944153\n",
            "Batch : 925, Loss : 0.5732871294021606\n",
            "Batch : 926, Loss : 0.588035523891449\n",
            "Batch : 927, Loss : 0.6937894225120544\n",
            "Batch : 928, Loss : 0.47734418511390686\n",
            "Batch : 929, Loss : 0.5381674766540527\n",
            "Batch : 930, Loss : 0.6132909059524536\n",
            "Batch : 931, Loss : 0.5435023307800293\n",
            "Batch : 932, Loss : 0.7875753045082092\n",
            "Batch : 933, Loss : 0.5156442523002625\n",
            "Batch : 934, Loss : 0.6731939315795898\n",
            "Batch : 935, Loss : 0.5816031694412231\n",
            "Batch : 936, Loss : 0.5270612239837646\n",
            "Batch : 937, Loss : 0.5809073448181152\n",
            "Batch : 938, Loss : 0.6932793259620667\n",
            "Training loss: 1.0027145996276758\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}